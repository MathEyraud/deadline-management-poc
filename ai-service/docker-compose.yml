version: '3.8'

services:
  ai-service:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./:/app  # Pour le développement, montage du code source
    environment:
      - MODEL_DIR=/app/models
      - DEFAULT_MODEL=mistral-7b-instruct-v0.2.Q4_K_M.gguf
      - GPU_LAYERS=0
      - MAX_TOKENS=512
      - TEMPERATURE=0.5
      - MAX_CONTEXT_ITEMS=10
      - MODEL_CTX_SIZE=2048
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 8G  # Limite de mémoire (ajuster selon les besoins)
          # Si vous utilisez une GPU
          # device_ids: ['0']  # ID de la GPU à utiliser
          # devices:
          #  - driver: nvidia
          #    capabilities: [compute, utility]

  # Intégration optionnelle avec le backend
  # backend:
  #   build:
  #     context: ../backend
  #     dockerfile: Dockerfile
  #   ports:
  #     - "3000:3000"
  #   environment:
  #     - AI_SERVICE_URL=http://ai-service:8000
  #   depends_on:
  #     - ai-service

  # Service de monitoring optionnel
  # prometheus:
  #   image: prom/prometheus
  #   ports:
  #     - "9090:9090"
  #   volumes:
  #     - ./prometheus:/etc/prometheus
  #   depends_on:
  #     - ai-service

  # grafana:
  #   image: grafana/grafana
  #   ports:
  #     - "3001:3000"
  #   depends_on:
  #     - prometheus