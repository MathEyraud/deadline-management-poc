"""
Service IA pour l'application de gestion d'√©ch√©ances
---------------------------------------------------
Ce service expose une API FastAPI qui permet d'interagir avec un mod√®le de langage (LLM)
d√©ploy√© localement via Ollama pour r√©pondre aux requ√™tes en langage naturel concernant la gestion d'√©ch√©ances.

Le service utilise:
- FastAPI pour l'API REST
- Pydantic pour la validation de donn√©es
- Ollama API pour l'inf√©rence du mod√®le
- asyncio pour le traitement asynchrone
"""

# Import des biblioth√®ques standard
import os
import json
import logging
import asyncio
import time
from typing import List, Dict, Optional, Any, Union
from datetime import datetime, timedelta, timezone

# Import des biblioth√®ques externes
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator
import uvicorn
import aiohttp

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("ai_service.log")
    ]
)
logger = logging.getLogger("ai_service")

# D√©finition des constantes
MAX_TOKENS = int(os.environ.get("MAX_TOKENS", "512"))
TEMPERATURE = float(os.environ.get("TEMPERATURE", "0.5"))
MAX_CONTEXT_ITEMS = int(os.environ.get("MAX_CONTEXT_ITEMS", "10"))

# Configuration Ollama
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "mistral")

# Initialisation de l'application FastAPI
app = FastAPI(
    title="Service IA pour la Gestion d'√âch√©ances",
    description="API pour interagir avec un LLM via Ollama pour la gestion d'√©ch√©ances",
    version="1.0.0"
)

# Ajout du middleware CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # √Ä restreindre en production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mod√®les Pydantic pour la validation des donn√©es

class MessageItem(BaseModel):
    """Un message dans une conversation"""
    role: str = Field(..., description="Le r√¥le de l'√©metteur du message (user ou assistant)")
    content: str = Field(..., description="Le contenu du message")
    
    @validator('role')
    def validate_role(cls, v):
        if v not in ["user", "assistant", "system"]:
            raise ValueError("Le r√¥le doit √™tre 'user', 'assistant' ou 'system'")
        return v

class DeadlineInfo(BaseModel):
    """Information sur une √©ch√©ance pour enrichir le contexte"""
    id: Optional[str] = None
    title: str
    description: Optional[str] = None
    deadlineDate: datetime
    status: str
    priority: str
    projectId: Optional[str] = None
    projectName: Optional[str] = None
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }

class UserInfo(BaseModel):
    """Informations sur l'utilisateur"""
    firstName: str
    lastName: str
    role: str
    department: Optional[str] = None

class ChatRequest(BaseModel):
    """Requ√™te pour le chat"""
    query: str = Field(..., description="La requ√™te en langage naturel")
    context: Optional[List[MessageItem]] = Field(None, description="Contexte de conversation pr√©c√©dent")
    deadlines: Optional[List[DeadlineInfo]] = Field(None, description="√âch√©ances √† inclure dans le contexte")
    user_id: Optional[str] = Field(None, description="ID de l'utilisateur")
    user_info: Optional[UserInfo] = Field(None, description="Informations sur l'utilisateur")

class PredictionRequest(BaseModel):
    """Requ√™te pour l'analyse pr√©dictive"""
    deadline_data: DeadlineInfo
    historical_data: Optional[List[DeadlineInfo]] = None
    user_id: Optional[str] = None
    user_info: Optional[UserInfo] = None

class ChatResponse(BaseModel):
    """R√©ponse du chat"""
    response: str = Field(..., description="La r√©ponse g√©n√©r√©e par le mod√®le")
    processing_time: float = Field(..., description="Temps de traitement en secondes")

class PredictionResponse(BaseModel):
    """R√©ponse d'analyse pr√©dictive"""
    completion_probability: float = Field(..., description="Probabilit√© de compl√©tion dans les d√©lais (0-1)")
    risk_factors: List[Dict[str, Any]] = Field(..., description="Facteurs de risque identifi√©s")
    recommendations: List[str] = Field(..., description="Recommandations pour am√©liorer les chances de compl√©tion")
    processing_time: float = Field(..., description="Temps de traitement en secondes")

# Fonction pour v√©rifier si Ollama est disponible
async def check_ollama_status():
    """V√©rifie si le service Ollama est disponible"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{OLLAMA_HOST}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    models = [model["name"] for model in data.get("models", [])]
                    logger.info(f"Mod√®les Ollama disponibles: {models}")
                    if OLLAMA_MODEL not in models:
                        logger.warning(f"Le mod√®le {OLLAMA_MODEL} n'est pas disponible dans Ollama")
                    return True, models
                else:
                    logger.error(f"Erreur lors de la v√©rification d'Ollama: {response.status}")
                    return False, []
    except Exception as e:
        logger.error(f"Impossible de se connecter √† Ollama: {e}")
        return False, []

# Formatage du prompt pour le mod√®le
def format_prompt_mistral(
    query: str, 
    context: List[MessageItem] = None, 
    deadlines: List[DeadlineInfo] = None,
    user_info: UserInfo = None
) -> List[Dict[str, str]]:
    """
    Formate le prompt pour Mistral en incluant le contexte et les informations sur les √©ch√©ances.
    
    Args:
        query: La requ√™te utilisateur actuelle
        context: Historique de conversation (messages pr√©c√©dents)
        deadlines: Liste des √©ch√©ances √† inclure dans le contexte
        user_info: Informations sur l'utilisateur
        
    Returns:
        Liste de messages au format attendu par Ollama
    """
    # Construction du prompt syst√®me
    system_prompt = """[Speak in french] Tu es un assistant IA sp√©cialis√© dans la gestion d'√©ch√©ances. Tu aides les utilisateurs √† g√©rer leurs √©ch√©ances, projets et d√©lais.
Tu analyses les informations fournies et tu donnes des conseils pertinents, des analyses et des pr√©dictions.
Ton objectif est d'aider l'utilisateur √† mieux organiser son travail, √† respecter ses d√©lais et √† prioriser ses t√¢ches.
Tu dois toujours r√©pondre en Fran√ßais et √™tre poli et professionnel.
Ne fais pas de suppositions sur les informations que tu n'as pas re√ßues.
Ne fais pas de remarques sur les informations que tu n'as pas re√ßues."""

    # Ajouter les informations utilisateur si disponibles
    if user_info:
        system_prompt += f"\n\nTu interagis avec {user_info.firstName} {user_info.lastName}, qui a le r√¥le de {user_info.role}"
        if user_info.department:
            system_prompt += f" dans le d√©partement {user_info.department}"
        system_prompt += "."

    # Ajouter les informations sur les √©ch√©ances au syst√®me s'il y en a
    if deadlines and len(deadlines) > 0:
        now = datetime.now(timezone.utc)
        system_prompt += "\n\nInformations sur les √©ch√©ances actuelles:"
        for i, deadline in enumerate(deadlines, 1):
            date_str = deadline.deadlineDate.strftime("%d/%m/%Y %H:%M")
            days_left = (deadline.deadlineDate - now).days
            status_emoji = "‚úÖ" if deadline.status.lower() in ["compl√©t√©e", "termin√©e", "completed"] else "‚è≥"
            priority_emoji = {
                "critique": "üî¥",
                "haute": "üü†",
                "moyenne": "üü°",
                "basse": "üü¢"
            }.get(deadline.priority.lower(), "‚ö™")
            
            project_info = f" (Projet: {deadline.projectName})" if deadline.projectName else ""
            
            system_prompt += f"\n{i}. {status_emoji} {priority_emoji} '{deadline.title}'{project_info} - √âch√©ance: {date_str} ({days_left} jours restants)"
            if deadline.description:
                system_prompt += f"\n   Description: {deadline.description}"

    # Construction des messages pour Ollama
    messages = []
    
    # Ajouter le message syst√®me
    messages.append({"role": "system", "content": system_prompt})
    
    # Ajouter le contexte de la conversation
    if context and len(context) > 0:
        logger.info(f"Utilisation d'un contexte de conversation avec {len(context)} messages")
        
        # Limiter le contexte au nombre max d'√©l√©ments si n√©cessaire
        limited_context = context[-MAX_CONTEXT_ITEMS:] if len(context) > MAX_CONTEXT_ITEMS else context
        
        # Si nous avons tronqu√© le contexte, on le log
        if len(context) > MAX_CONTEXT_ITEMS:
            logger.info(f"Contexte tronqu√© de {len(context)} √† {MAX_CONTEXT_ITEMS} messages")
        
        # Ajouter chaque message du contexte
        for msg in limited_context:
            messages.append({"role": msg.role, "content": msg.content})
    else:
        logger.info("Aucun contexte de conversation fourni")
    
    # Ajouter la requ√™te actuelle
    messages.append({"role": "user", "content": query})
    
    return messages

# Fonction de g√©n√©ration asynchrone via Ollama API
async def generate_response_ollama(messages) -> str:
    """
    G√©n√®re une r√©ponse en utilisant l'API Ollama.
    
    Args:
        messages: Liste de messages format√©s {"role": "...", "content": "..."}
        
    Returns:
        Texte de la r√©ponse g√©n√©r√©e
    """
    try:
        url = f"{OLLAMA_HOST}/api/chat"
        payload = {
            "model": OLLAMA_MODEL,
            "messages": messages,
            "options": {
                "temperature": TEMPERATURE,
                "num_predict": MAX_TOKENS
            },
            "stream": False  # Important: d√©sactiver le streaming pour obtenir une r√©ponse JSON compl√®te
        }
        
        logger.info(f"Envoi de la requ√™te √† Ollama: {url}")
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload) as response:
                if response.status == 200:
                    # Lire le contenu de la r√©ponse comme texte puis parser en JSON
                    response_text = await response.text()
                    try:
                        data = json.loads(response_text)
                        return data["message"]["content"]
                    except json.JSONDecodeError:
                        # Si nous avons un format NDJSON (ligne par ligne), traiter chaque ligne
                        logger.info("R√©ponse au format NDJSON d√©tect√©e, traitement ligne par ligne")
                        lines = response_text.strip().split('\n')
                        if lines and len(lines) > 0:
                            # Prendre la derni√®re ligne qui contient g√©n√©ralement la r√©ponse compl√®te
                            try:
                                last_data = json.loads(lines[-1])
                                if "message" in last_data and "content" in last_data["message"]:
                                    return last_data["message"]["content"]
                            except:
                                pass
                        
                        # Si tout √©choue, renvoyer le texte brut
                        logger.warning(f"Impossible de parser la r√©ponse JSON, renvoi du texte brut")
                        return response_text
                else:
                    error_text = await response.text()
                    logger.error(f"Erreur Ollama {response.status}: {error_text}")
                    raise HTTPException(
                        status_code=status.HTTP_502_BAD_GATEWAY,
                        detail=f"Erreur lors de la g√©n√©ration via Ollama: {error_text}"
                    )
    except aiohttp.ClientError as e:
        logger.error(f"Erreur de connexion √† Ollama: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"Impossible de se connecter √† Ollama: {str(e)}"
        )
    except Exception as e:
        logger.error(f"Erreur inattendue avec Ollama: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Erreur inattendue: {str(e)}"
        )

# V√©rification de l'√©tat d'Ollama au d√©marrage
@app.on_event("startup")
async def startup_event():
    """V√©rification de l'√©tat d'Ollama au d√©marrage"""
    ollama_available, models = await check_ollama_status()
    if not ollama_available:
        logger.warning("Ollama n'est pas disponible. Le service fonctionnera en mode d√©grad√©.")
    else:
        logger.info(f"Ollama est disponible avec les mod√®les: {models}")

# Endpoint pour le healthcheck
@app.get("/health")
async def health_check():
    """Endpoint de healthcheck pour v√©rifier que le service est actif"""
    ollama_available, models = await check_ollama_status()
    
    return {
        "status": "ok" if ollama_available else "d√©grad√©",
        "ollama": {
            "available": ollama_available,
            "models": models,
            "selected_model": OLLAMA_MODEL,
            "host": OLLAMA_HOST
        },
        "version": "1.0.0"
    }

# Endpoint pour le chat
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Endpoint pour interagir avec le mod√®le en mode chat.
    
    - query: La question ou instruction de l'utilisateur
    - context: Historique de conversation (messages pr√©c√©dents)
    - deadlines: Informations sur les √©ch√©ances pour enrichir le contexte
    - user_id: Identifiant de l'utilisateur
    - user_info: Informations sur l'utilisateur
    """
    start_time = time.time()
    
    try:
        # Log du nombre de messages de contexte re√ßus
        context_count = len(request.context) if request.context else 0
        logger.info(f"Requ√™te chat re√ßue avec {context_count} messages de contexte")
        
        # Formater les messages pour Ollama
        messages = format_prompt_mistral(
            query=request.query,
            context=request.context,
            deadlines=request.deadlines,
            user_info=request.user_info
        )
        
        # G√©n√©rer la r√©ponse via Ollama
        response_text = await generate_response_ollama(messages)
        
        # Calculer le temps de traitement
        processing_time = time.time() - start_time
        logger.info(f"R√©ponse g√©n√©r√©e en {processing_time:.2f} secondes")
        
        return ChatResponse(
            response=response_text,
            processing_time=processing_time
        )
    
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration de r√©ponse: {e}")
        if isinstance(e, HTTPException):
            raise e
        raise HTTPException(status_code=500, detail=f"Erreur lors de la g√©n√©ration de r√©ponse: {str(e)}")

# Endpoint pour les pr√©dictions
@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """
    Endpoint pour l'analyse pr√©dictive des √©ch√©ances.
    
    Utilise le mod√®le pour pr√©dire la probabilit√© de compl√©tion d'une √©ch√©ance
    et fournir des recommandations.
    """
    start_time = time.time()
    now = datetime.now(timezone.utc)
    
    try:
        # Formater la demande pour le mod√®le
        date_str = request.deadline_data.deadlineDate.strftime("%d/%m/%Y %H:%M")
        days_left = (request.deadline_data.deadlineDate - now).days
        
        # Personnalisation selon l'utilisateur
        user_greeting = ""
        if request.user_info:
            user_greeting = f"Analyse pour {request.user_info.firstName} {request.user_info.lastName} ({request.user_info.role}): "
        
        query = f"""{user_greeting}Analyse cette √©ch√©ance et pr√©dit sa probabilit√© de compl√©tion dans les d√©lais.
        
√âch√©ance: {request.deadline_data.title}
Description: {request.deadline_data.description or 'Non sp√©cifi√©e'}
Date d'√©ch√©ance: {date_str} (dans {days_left} jours)
Priorit√©: {request.deadline_data.priority}
Statut: {request.deadline_data.status}

Bas√© sur ces informations:
1. Quelle est la probabilit√© (entre 0 et 1) que cette √©ch√©ance soit compl√©t√©e √† temps?
2. Quels sont les facteurs de risque?
3. Quelles recommandations donnerais-tu pour augmenter les chances de compl√©tion?

R√©ponds au format JSON avec les champs "completion_probability", "risk_factors" et "recommendations".
"""
        
        # G√©n√©rer la r√©ponse
        messages = [
            {"role": "system", "content": "Tu es un assistant sp√©cialis√© dans l'analyse pr√©dictive d'√©ch√©ances"},
            {"role": "user", "content": query}
        ]
        response_text = await generate_response_ollama(messages)
        
        # Extraire les informations de la r√©ponse (recherche d'un JSON valide)
        json_start = response_text.find("{")
        json_end = response_text.rfind("}")
        
        if json_start != -1 and json_end != -1:
            json_str = response_text[json_start:json_end+1]
            try:
                prediction_data = json.loads(json_str)
                
                # Valider et normaliser les donn√©es
                completion_probability = float(prediction_data.get("completion_probability", 0.5))
                # Limiter entre 0 et 1
                completion_probability = max(0, min(1, completion_probability))
                
                risk_factors = prediction_data.get("risk_factors", [])
                if isinstance(risk_factors, str):
                    risk_factors = [{"factor": risk_factors, "impact": "medium"}]
                elif isinstance(risk_factors, list) and all(isinstance(rf, str) for rf in risk_factors):
                    risk_factors = [{"factor": rf, "impact": "medium"} for rf in risk_factors]
                
                recommendations = prediction_data.get("recommendations", [])
                if isinstance(recommendations, str):
                    recommendations = [recommendations]
                
                # Calculer le temps de traitement
                processing_time = time.time() - start_time
                
                return PredictionResponse(
                    completion_probability=completion_probability,
                    risk_factors=risk_factors,
                    recommendations=recommendations,
                    processing_time=processing_time
                )
            
            except json.JSONDecodeError:
                # Fallback si le JSON n'est pas valide
                logger.warning("Impossible de parser le JSON du mod√®le, utilisation d'une analyse heuristique")
        
        # Fallback : analyse heuristique si le JSON n'est pas valide ou n'est pas pr√©sent
        # Extraire probabilit√© avec une regex basique
        import re
        prob_match = re.search(r"probabilit√©.*?(\d+(?:\.\d+)?)", response_text, re.IGNORECASE)
        probability = float(prob_match.group(1))/100 if prob_match else 0.5
        
        # Extraire risques et recommandations par des heuristiques simples
        factors_section = extract_section(response_text, ["facteurs", "risques"])
        factors = parse_list_items(factors_section) if factors_section else ["D√©lai serr√©"]
        
        recommendations_section = extract_section(response_text, ["recommandations", "suggestions"])
        recommendations = parse_list_items(recommendations_section) if recommendations_section else ["Planifier les √©tapes"]
        
        risk_factors = [{"factor": factor, "impact": "medium"} for factor in factors]
        
        # Calculer le temps de traitement
        processing_time = time.time() - start_time
        
        return PredictionResponse(
            completion_probability=probability,
            risk_factors=risk_factors,
            recommendations=recommendations,
            processing_time=processing_time
        )
    
    except Exception as e:
        logger.error(f"Erreur lors de la pr√©diction: {e}")
        if isinstance(e, HTTPException):
            raise e
        raise HTTPException(status_code=500, detail=f"Erreur lors de la pr√©diction: {str(e)}")

# Fonctions utilitaires pour le fallback
def extract_section(text, keywords):
    """Extrait une section bas√©e sur des mots-cl√©s"""
    import re
    patterns = [
        rf"({'|'.join(keywords)}).*?\n(.*?)(?:\n\n|\n[A-Z0-9]|$)",
        rf"({'|'.join(keywords)}).*?:(.*?)(?:\n\n|\n[A-Z0-9]|$)"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(2).strip()
    return ""

def parse_list_items(text):
    """Parse les √©l√©ments d'une liste depuis un texte"""
    import re
    # Essayer d'extraire des items num√©rot√©s ou avec puces
    items = re.findall(r"(?:^|\n)[\s]*(?:\d+\.|-|\*|\‚Ä¢)[\s]*(.*?)(?:\n|$)", text)
    
    # Si aucun item n'est trouv√©, diviser par lignes
    if not items:
        items = [line.strip() for line in text.split("\n") if line.strip()]
    
    # S'il n'y a toujours rien, retourner le texte comme un seul √©l√©ment
    if not items:
        items = [text]
    
    return items

# Endpoint pour tester directement Ollama
@app.post("/test_ollama")
async def test_ollama(prompt: str = "Comment am√©liorer la gestion d'√©ch√©ances?"):
    """Endpoint de test pour v√©rifier la connexion √† Ollama"""
    try:
        messages = [
            {"role": "user", "content": prompt}
        ]
        response = await generate_response_ollama(messages)
        return {"success": True, "response": response}
    except Exception as e:
        return {"success": False, "error": str(e)}

# Point d'entr√©e
if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )